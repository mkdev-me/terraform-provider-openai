---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "openai_fine_tuning_job Resource - terraform-provider-openai"
subcategory: ""
description: |-
  
---

# openai_fine_tuning_job (Resource)



## Example Usage

```terraform
# Upload training data file
resource "openai_file" "training_data" {
  file    = "training_data.jsonl"
  purpose = "fine-tune"
}

# Upload validation data file (optional)
resource "openai_file" "validation_data" {
  file    = "validation_data.jsonl"
  purpose = "fine-tune"
}

# Create a fine-tuning job
resource "openai_fine_tuning_job" "custom_model" {
  training_file = openai_file.training_data.id
  model         = "gpt-3.5-turbo-0125" # Base model to fine-tune

  # Optional: Add validation file for better model evaluation
  validation_file = openai_file.validation_data.id

  # Optional: Configure hyperparameters
  hyperparameters {
    n_epochs                 = 3
    batch_size               = 4
    learning_rate_multiplier = 0.1
  }

  # Optional: Set a custom suffix for the fine-tuned model name
  suffix = "customer-support-v2"

  # Optional: Add metadata
  metadata = {
    department = "customer-service"
    use_case   = "support-automation"
    version    = "2.0"
    trained_by = "ml-team"
  }
}

# Create another fine-tuning job with minimal configuration
resource "openai_fine_tuning_job" "simple_model" {
  training_file = openai_file.training_data.id
  model         = "gpt-3.5-turbo-0125"

  # Use default hyperparameters
  suffix = "basic-v1"
}

# Create a fine-tuning job with specific seed for reproducibility
resource "openai_fine_tuning_job" "reproducible_model" {
  training_file = openai_file.training_data.id
  model         = "gpt-3.5-turbo-0125"

  hyperparameters {
    n_epochs                 = 5
    batch_size               = 8
    learning_rate_multiplier = 0.05
  }

  # Set seed for reproducible results
  seed = 42

  suffix = "reproducible-v1"

  metadata = {
    experiment_id = "EXP-2024-001"
    reproducible  = "true"
  }
}

# Output the fine-tuning job details
output "fine_tuning_job_id" {
  value       = openai_fine_tuning_job.custom_model.id
  description = "The ID of the fine-tuning job"
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `model` (String) The base model to fine-tune (e.g., gpt-3.5-turbo, gpt-4o-mini)
- `training_file` (String) The ID of the uploaded file containing training data

### Optional

- `cancel_after_timeout` (Number) Automatically cancel the job after this many seconds
- `hyperparameters` (Block List, Max: 1) Deprecated: Use method instead. Hyperparameters for the fine-tuning job (see [below for nested schema](#nestedblock--hyperparameters))
- `integrations` (Block List) List of integrations to enable for the fine-tuning job (see [below for nested schema](#nestedblock--integrations))
- `metadata` (Map of String) Key-value pairs attached to the fine-tuning job
- `method` (Block List, Max: 1) The method used for fine-tuning (see [below for nested schema](#nestedblock--method))
- `seed` (Number) Seed for reproducibility
- `suffix` (String) A string of up to 64 characters added to your fine-tuned model name
- `validation_file` (String) The ID of the uploaded file containing validation data

### Read-Only

- `created_at` (Number) Timestamp when the fine-tuning job was created
- `fine_tuned_model` (String) The name of the fine-tuned model
- `finished_at` (Number) Timestamp when the fine-tuning job was completed
- `id` (String) The ID of this resource.
- `last_updated` (String) Timestamp of when this resource was last updated
- `organization_id` (String) The organization ID the fine-tuning job belongs to
- `result_files` (List of String) Result files from the fine-tuning job
- `status` (String) Status of the fine-tuning job
- `trained_tokens` (Number) The number of tokens trained during the fine-tuning job
- `validation_loss` (Number) The validation loss for the fine-tuning job

<a id="nestedblock--hyperparameters"></a>
### Nested Schema for `hyperparameters`

Optional:

- `batch_size` (Number) Number of examples in each batch
- `learning_rate_multiplier` (Number) Learning rate multiplier
- `n_epochs` (Number) Number of epochs to train for


<a id="nestedblock--integrations"></a>
### Nested Schema for `integrations`

Required:

- `type` (String) The type of integration (e.g., wandb)

Optional:

- `wandb` (Block List, Max: 1) Configuration for Weights & Biases integration (see [below for nested schema](#nestedblock--integrations--wandb))

<a id="nestedblock--integrations--wandb"></a>
### Nested Schema for `integrations.wandb`

Required:

- `project` (String) The W&B project name

Optional:

- `name` (String) The W&B run name
- `tags` (List of String) Tags for the W&B run



<a id="nestedblock--method"></a>
### Nested Schema for `method`

Required:

- `type` (String) The type of fine-tuning method (supervised or dpo)

Optional:

- `dpo` (Block List, Max: 1) Configuration for DPO fine-tuning (see [below for nested schema](#nestedblock--method--dpo))
- `supervised` (Block List, Max: 1) Configuration for supervised fine-tuning (see [below for nested schema](#nestedblock--method--supervised))

<a id="nestedblock--method--dpo"></a>
### Nested Schema for `method.dpo`

Optional:

- `hyperparameters` (Block List, Max: 1) Hyperparameters for DPO fine-tuning (see [below for nested schema](#nestedblock--method--dpo--hyperparameters))

<a id="nestedblock--method--dpo--hyperparameters"></a>
### Nested Schema for `method.dpo.hyperparameters`

Optional:

- `beta` (Number) Beta parameter for DPO



<a id="nestedblock--method--supervised"></a>
### Nested Schema for `method.supervised`

Optional:

- `hyperparameters` (Block List, Max: 1) Hyperparameters for supervised fine-tuning (see [below for nested schema](#nestedblock--method--supervised--hyperparameters))

<a id="nestedblock--method--supervised--hyperparameters"></a>
### Nested Schema for `method.supervised.hyperparameters`

Optional:

- `batch_size` (Number) Number of examples in each batch
- `learning_rate_multiplier` (Number) Learning rate multiplier
- `n_epochs` (Number) Number of epochs to train for

## Import

Import is supported using the following syntax:

```shell
#!/bin/bash
# Import existing OpenAI fine-tuning job
terraform import openai_fine_tuning_job.example ftjob-abc123def456
```
