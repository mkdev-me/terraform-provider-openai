{"format_version":"1.0","provider_schemas":{"registry.terraform.io/fjcorp/openai":{"provider":{"version":0,"block":{"attributes":{"api_key":{"type":"string","description":"The OpenAI API key. This can also be set using the OPENAI_API_KEY environment variable.","description_kind":"plain","optional":true,"sensitive":true},"base_url":{"type":"string","description":"The base URL for OpenAI's API. This can also be set using the OPENAI_BASE_URL environment variable.","description_kind":"plain","optional":true},"organization":{"type":"string","description":"The OpenAI organization. This can also be set using the OPENAI_ORGANIZATION environment variable.","description_kind":"plain","optional":true}},"description_kind":"plain"}},"resource_schemas":{"openai_chat_completion":{"version":0,"block":{"attributes":{"chat_completion_id":{"type":"string","description":"The ID of the chat completion","description_kind":"plain","computed":true},"created":{"type":"number","description":"The Unix timestamp (in seconds) of when the chat completion was created","description_kind":"plain","computed":true},"frequency_penalty":{"type":"number","description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far","description_kind":"plain","optional":true},"function_call":{"type":"string","description":"Controls how the model responds to function calls. 'none' means the model doesn't call a function, 'auto' means the model can pick between calling a function or generating a message","description_kind":"plain","optional":true},"id":{"type":"string","description_kind":"plain","optional":true,"computed":true},"logit_bias":{"type":["map","number"],"description":"Modify the likelihood of specified tokens appearing in the completion","description_kind":"plain","optional":true},"max_tokens":{"type":"number","description":"The maximum number of tokens to generate in the chat completion","description_kind":"plain","optional":true},"model":{"type":"string","description":"ID of the model to use for the chat completion","description_kind":"plain","required":true},"model_used":{"type":"string","description":"The model used for the chat completion","description_kind":"plain","computed":true},"n":{"type":"number","description":"How many chat completion choices to generate for each input message","description_kind":"plain","optional":true},"object":{"type":"string","description":"The object type, which is always 'chat.completion'","description_kind":"plain","computed":true},"presence_penalty":{"type":"number","description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far","description_kind":"plain","optional":true},"project_id":{"type":"string","description":"The project to use for this request","description_kind":"plain","optional":true},"stop":{"type":["list","string"],"description":"Up to 4 sequences where the API will stop generating further tokens","description_kind":"plain","optional":true},"stream":{"type":"bool","description":"Whether to stream back partial progress","description_kind":"plain","optional":true},"temperature":{"type":"number","description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic","description_kind":"plain","optional":true},"top_p":{"type":"number","description":"Nuclear sampling: consider the results of the tokens with top_p probability mass. Range from 0 to 1","description_kind":"plain","optional":true},"usage":{"type":["map","number"],"description":"Usage statistics for the chat completion request","description_kind":"plain","computed":true},"user":{"type":"string","description":"A unique identifier representing your end-user","description_kind":"plain","optional":true}},"block_types":{"choices":{"nesting_mode":"list","block":{"attributes":{"finish_reason":{"type":"string","description":"The reason the model stopped generating text","description_kind":"plain","computed":true},"index":{"type":"number","description":"The index of the choice in the list of choices","description_kind":"plain","computed":true},"message":{"type":["list",["object",{"content":"string","function_call":["list",["object",{"arguments":"string","name":"string"}]],"role":"string"}]],"description":"The message generated by the model","description_kind":"plain","computed":true}},"description":"The list of chat completion choices the model generated","description_kind":"plain"}},"functions":{"nesting_mode":"list","block":{"attributes":{"description":{"type":"string","description":"A description of what the function does","description_kind":"plain","optional":true},"name":{"type":"string","description":"The name of the function","description_kind":"plain","required":true},"parameters":{"type":"string","description":"The parameters the function accepts, described as a JSON Schema object","description_kind":"plain","required":true}},"description":"A list of functions the model may generate JSON inputs for","description_kind":"plain"}},"messages":{"nesting_mode":"list","block":{"attributes":{"content":{"type":"string","description":"The content of the message","description_kind":"plain","required":true},"name":{"type":"string","description":"The name of the author of this message. Required if role is 'function'","description_kind":"plain","optional":true},"role":{"type":"string","description":"The role of the message author. One of 'system', 'user', 'assistant', or 'function'","description_kind":"plain","required":true}},"block_types":{"function_call":{"nesting_mode":"list","block":{"attributes":{"arguments":{"type":"string","description":"The arguments to call the function with, as a JSON string","description_kind":"plain","required":true},"name":{"type":"string","description":"The name of the function to call","description_kind":"plain","required":true}},"description":"The name and arguments of a function that should be called, as generated by the model","description_kind":"plain"},"max_items":1}},"description":"A list of messages comprising the conversation so far","description_kind":"plain"},"min_items":1}},"description_kind":"plain"}},"openai_file":{"version":0,"block":{"attributes":{"bytes":{"type":"number","description":"The size of the file in bytes","description_kind":"plain","computed":true},"created_at":{"type":"number","description":"The timestamp for when the file was created","description_kind":"plain","computed":true},"file":{"type":"string","description":"Path to the file to upload","description_kind":"plain","required":true},"filename":{"type":"string","description":"The name of the file","description_kind":"plain","computed":true},"id":{"type":"string","description":"The identifier of the file","description_kind":"plain","computed":true},"project_id":{"type":"string","description":"The project ID to associate this file with (for Terraform reference only, not sent to OpenAI API)","description_kind":"plain","optional":true},"purpose":{"type":"string","description":"The purpose of the file. Can be 'fine-tune', 'assistants', 'vision', or 'batch'.","description_kind":"plain","required":true},"status":{"type":"string","description":"The status of the file","description_kind":"plain","computed":true},"status_details":{"type":"string","description":"Additional details about the status of the file","description_kind":"plain","computed":true}},"description_kind":"plain"}},"openai_model_response":{"version":0,"block":{"attributes":{"created":{"type":"number","description":"Unix timestamp when the response was created.","description_kind":"plain","computed":true},"finish_reason":{"type":"string","description":"Reason why the response finished (e.g., stop, length, content).","description_kind":"plain","computed":true},"frequency_penalty":{"type":"number","description":"Penalty for token frequency between -2.0 and 2.0.","description_kind":"plain","optional":true},"id":{"type":"string","description":"Unique identifier for this response.","description_kind":"plain","computed":true},"include":{"type":["list","string"],"description":"Optional fields to include in the response.","description_kind":"plain","optional":true},"input":{"type":"string","description":"The input text to generate a response for.","description_kind":"plain","required":true},"instructions":{"type":"string","description":"Optional instructions to guide the model.","description_kind":"plain","optional":true},"max_output_tokens":{"type":"number","description":"The maximum number of tokens to generate.","description_kind":"plain","optional":true},"model":{"type":"string","description":"ID of the model to use (e.g., 'gpt-4o', 'gpt-4-turbo').","description_kind":"plain","required":true},"object":{"type":"string","description":"Object type (usually 'model_response').","description_kind":"plain","computed":true},"output":{"type":["map","string"],"description":"The generated output containing text and token count.","description_kind":"plain","computed":true},"presence_penalty":{"type":"number","description":"Penalty for token presence between -2.0 and 2.0.","description_kind":"plain","optional":true},"stop_sequences":{"type":["list","string"],"description":"Optional list of sequences where the API will stop generating further tokens.","description_kind":"plain","optional":true},"temperature":{"type":"number","description":"Sampling temperature between 0 and 2. Higher values mean more randomness.","description_kind":"plain","optional":true},"top_k":{"type":"number","description":"Top-k sampling parameter. Only consider top k tokens.","description_kind":"plain","optional":true},"top_p":{"type":"number","description":"Nucleus sampling parameter. Top probability mass to consider.","description_kind":"plain","optional":true},"usage":{"type":["map","string"],"description":"Token usage statistics for the request.","description_kind":"plain","computed":true},"user":{"type":"string","description":"A unique identifier representing the end-user, to help track and detect abuse.","description_kind":"plain","optional":true}},"description":"Generates text from OpenAI models using the model response API endpoint.","description_kind":"plain"}},"openai_moderation":{"version":0,"block":{"attributes":{"_api_response":{"type":"string","description":"The full API response from OpenAI.","description_kind":"plain","computed":true},"categories":{"type":["map","bool"],"description":"Map of category names to boolean values indicating if the content violates that category (first result if multiple inputs).","description_kind":"plain","computed":true},"category_scores":{"type":["map","number"],"description":"Map of category names to scores (0.0 to 1.0) indicating the confidence of the model (first result if multiple inputs).","description_kind":"plain","computed":true},"flagged":{"type":"bool","description":"Whether the content violates OpenAI's usage policies (first result if multiple inputs).","description_kind":"plain","computed":true},"id":{"type":"string","description":"The ID of the moderation result.","description_kind":"plain","computed":true},"input":{"type":"string","description":"Input (or inputs) to classify. Can be a single string, an array of strings, or an array of multi-modal input objects.","description_kind":"plain","required":true},"model":{"type":"string","description":"The content moderation model you would like to use.","description_kind":"plain","optional":true},"results":{"type":["list",["object",{"categories":["map","bool"],"category_applied_input_types":["map",["list","string"]],"category_scores":["map","number"],"flagged":"bool"}]],"description":"The full results of the moderation API call.","description_kind":"plain","computed":true}},"description_kind":"plain"}},"openai_vector_store":{"version":0,"block":{"attributes":{"created_at":{"type":"number","description":"The timestamp for when the vector store was created.","description_kind":"plain","computed":true},"file_count":{"type":"number","description":"The number of files in the vector store.","description_kind":"plain","computed":true},"file_ids":{"type":["list","string"],"description":"The list of file IDs to use in the vector store.","description_kind":"plain","optional":true},"id":{"type":"string","description":"The ID of the vector store.","description_kind":"plain","computed":true},"metadata":{"type":["map","string"],"description":"Set of key-value pairs that can be attached to the vector store.","description_kind":"plain","optional":true},"name":{"type":"string","description":"The name of the vector store.","description_kind":"plain","required":true},"object":{"type":"string","description":"The object type (always 'vector_store').","description_kind":"plain","computed":true},"status":{"type":"string","description":"The current status of the vector store.","description_kind":"plain","computed":true}},"block_types":{"chunking_strategy":{"nesting_mode":"list","block":{"attributes":{"type":{"type":"string","description":"The type of chunking strategy (auto or static).","description_kind":"plain","required":true}},"description":"The chunking strategy used to chunk the files.","description_kind":"plain"},"max_items":1},"expires_after":{"nesting_mode":"list","block":{"attributes":{"anchor":{"type":"string","description":"The anchor time for the expiration (usually 'now').","description_kind":"plain","required":true},"days":{"type":"number","description":"Number of days after which the vector store should expire.","description_kind":"plain","optional":true}},"description":"The expiration policy for the vector store.","description_kind":"plain"},"max_items":1}},"description_kind":"plain"}},"openai_vector_store_file":{"version":0,"block":{"attributes":{"attributes":{"type":["list","string"],"description":"Dynamic description or tags for the file in the vector store.","description_kind":"plain","optional":true},"created_at":{"type":"number","description":"The timestamp for when the file was added to the vector store.","description_kind":"plain","computed":true},"file_id":{"type":"string","description":"The ID of the file to add to the vector store.","description_kind":"plain","required":true},"id":{"type":"string","description":"The ID of the vector store file.","description_kind":"plain","computed":true},"object":{"type":"string","description":"The object type (always 'vector_store.file').","description_kind":"plain","computed":true},"status":{"type":"string","description":"The current status of the file in the vector store.","description_kind":"plain","computed":true},"vector_store_id":{"type":"string","description":"The ID of the vector store.","description_kind":"plain","required":true}},"block_types":{"chunking_strategy":{"nesting_mode":"list","block":{"attributes":{"max_tokens":{"type":"number","description":"The maximum tokens per chunk for semantic chunking strategy.","description_kind":"plain","optional":true},"size":{"type":"number","description":"The size in characters for fixed chunking strategy.","description_kind":"plain","optional":true},"type":{"type":"string","description":"The type of chunking strategy (auto, fixed, or semantic).","description_kind":"plain","required":true}},"description":"The chunking strategy used to chunk the file.","description_kind":"plain"},"max_items":1}},"description_kind":"plain"}},"openai_vector_store_file_batch":{"version":0,"block":{"attributes":{"attributes":{"type":["list","string"],"description":"Dynamic description or tags for the files in the vector store.","description_kind":"plain","optional":true},"created_at":{"type":"number","description":"The timestamp for when the files were added to the vector store.","description_kind":"plain","computed":true},"file_ids":{"type":["list","string"],"description":"The list of file IDs to add to the vector store.","description_kind":"plain","required":true},"id":{"type":"string","description":"The ID of the vector store file batch operation.","description_kind":"plain","computed":true},"object":{"type":"string","description":"The object type (always 'vector_store.file.batch').","description_kind":"plain","computed":true},"status":{"type":"string","description":"The current status of the file batch operation.","description_kind":"plain","computed":true},"vector_store_id":{"type":"string","description":"The ID of the vector store to add the files to.","description_kind":"plain","required":true}},"block_types":{"chunking_strategy":{"nesting_mode":"list","block":{"attributes":{"max_tokens":{"type":"number","description":"The maximum tokens per chunk for semantic chunking strategy.","description_kind":"plain","optional":true},"size":{"type":"number","description":"The size in characters for fixed chunking strategy.","description_kind":"plain","optional":true},"type":{"type":"string","description":"The type of chunking strategy (auto, fixed, or semantic).","description_kind":"plain","required":true}},"description":"The chunking strategy used to chunk the files.","description_kind":"plain"},"max_items":1}},"description_kind":"plain"}}}}}}
