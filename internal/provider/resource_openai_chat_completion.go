package provider

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"

	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
)

// ChatCompletionResponse represents the API response for chat completions.
// It contains the generated response, model information, and usage statistics.
type ChatCompletionResponse struct {
	ID      string                 `json:"id"`      // Unique identifier for the completion
	Object  string                 `json:"object"`  // Type of object (e.g., "chat.completion")
	Created int                    `json:"created"` // Unix timestamp when the completion was created
	Model   string                 `json:"model"`   // Model used for the completion
	Choices []ChatCompletionChoice `json:"choices"` // List of possible completions
	Usage   ChatCompletionUsage    `json:"usage"`   // Token usage statistics
}

// ChatCompletionChoice represents a single completion option from the model.
// It contains the generated message and information about why the completion finished.
type ChatCompletionChoice struct {
	Index        int                   `json:"index"`         // Index of the choice in the list
	Message      ChatCompletionMessage `json:"message"`       // The generated message
	FinishReason string                `json:"finish_reason"` // Reason why the completion finished
}

// ChatCompletionMessage represents a message in the chat completion.
// It can be either a user message, assistant message, or system message.
type ChatCompletionMessage struct {
	Role         string            `json:"role"`                    // Role of the message sender (system, user, assistant)
	Content      string            `json:"content"`                 // Content of the message
	FunctionCall *ChatFunctionCall `json:"function_call,omitempty"` // Optional function call
	Name         string            `json:"name,omitempty"`          // Optional name of the message sender
}

// ChatFunctionCall represents a function call generated by the model.
// It contains the function name and arguments to be passed to the function.
type ChatFunctionCall struct {
	Name      string `json:"name"`      // Name of the function to call
	Arguments string `json:"arguments"` // JSON string containing function arguments
}

// ChatCompletionUsage represents token usage statistics for the completion request.
// It tracks the number of tokens used in the prompt and completion.
type ChatCompletionUsage struct {
	PromptTokens     int `json:"prompt_tokens"`     // Number of tokens in the prompt
	CompletionTokens int `json:"completion_tokens"` // Number of tokens in the completion
	TotalTokens      int `json:"total_tokens"`      // Total number of tokens used
}

// ChatCompletionRequest represents the request payload for creating a chat completion.
// It specifies the model, messages, and various parameters to control the completion.
type ChatCompletionRequest struct {
	Model            string                  `json:"model"`                       // ID of the model to use
	Messages         []ChatCompletionMessage `json:"messages"`                    // List of messages in the conversation
	Functions        []ChatFunction          `json:"functions,omitempty"`         // Optional list of available functions
	FunctionCall     interface{}             `json:"function_call,omitempty"`     // Optional function call configuration
	Temperature      float64                 `json:"temperature,omitempty"`       // Sampling temperature
	TopP             float64                 `json:"top_p,omitempty"`             // Nucleus sampling parameter
	N                int                     `json:"n,omitempty"`                 // Number of completions to generate
	Stream           bool                    `json:"stream,omitempty"`            // Whether to stream the response
	Stop             []string                `json:"stop,omitempty"`              // Optional stop sequences
	MaxTokens        int                     `json:"max_tokens,omitempty"`        // Maximum tokens to generate
	PresencePenalty  float64                 `json:"presence_penalty,omitempty"`  // Presence penalty parameter
	FrequencyPenalty float64                 `json:"frequency_penalty,omitempty"` // Frequency penalty parameter
	LogitBias        map[string]float64      `json:"logit_bias,omitempty"`        // Optional token bias
	User             string                  `json:"user,omitempty"`              // Optional user identifier
	Store            bool                    `json:"store,omitempty"`             // Whether to store the completion
	Metadata         map[string]string       `json:"metadata,omitempty"`          // Optional metadata for filtering
}

// ChatFunction represents a function that can be called by the model.
// It contains the function name, description, and parameter schema.
type ChatFunction struct {
	Name        string          `json:"name"`                  // Name of the function
	Description string          `json:"description,omitempty"` // Optional function description
	Parameters  json.RawMessage `json:"parameters"`            // JSON schema for function parameters
}

// resourceOpenAIChatCompletion defines the schema and CRUD operations for OpenAI chat completions.
// This resource allows users to generate chat completions using OpenAI's language models.
// It supports various models and provides options for controlling the completion behavior.
func resourceOpenAIChatCompletion() *schema.Resource {
	return &schema.Resource{
		CreateContext: resourceOpenAIChatCompletionCreate,
		ReadContext:   resourceOpenAIChatCompletionRead,
		UpdateContext: resourceOpenAIChatCompletionUpdate,
		DeleteContext: resourceOpenAIChatCompletionDelete,
		Importer: &schema.ResourceImporter{
			StateContext: resourceOpenAIChatCompletionImport,
		},
		CustomizeDiff: customizeDiffForChatCompletion,
		Schema: map[string]*schema.Schema{
			"model": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: "ID of the model to use for the chat completion",
			},
			"messages": {
				Type:        schema.TypeList,
				Required:    true,
				Description: "A list of messages comprising the conversation so far",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"role": {
							Type:         schema.TypeString,
							Required:     true,
							ValidateFunc: validation.StringInSlice([]string{"system", "user", "assistant", "function"}, false),
							Description:  "The role of the message author. One of 'system', 'user', 'assistant', or 'function'",
						},
						"content": {
							Type:        schema.TypeString,
							Required:    true,
							Description: "The content of the message",
						},
						"name": {
							Type:        schema.TypeString,
							Optional:    true,
							Description: "The name of the author of this message. Required if role is 'function'",
						},
						"function_call": {
							Type:        schema.TypeList,
							Optional:    true,
							MaxItems:    1,
							Description: "The name and arguments of a function that should be called, as generated by the model",
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"name": {
										Type:        schema.TypeString,
										Required:    true,
										Description: "The name of the function to call",
									},
									"arguments": {
										Type:        schema.TypeString,
										Required:    true,
										Description: "The arguments to call the function with, as a JSON string",
									},
								},
							},
						},
					},
				},
			},
			"functions": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: "A list of functions the model may generate JSON inputs for",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"name": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: "The name of the function",
						},
						"description": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: "A description of what the function does",
						},
						"parameters": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: "The parameters the function accepts, described as a JSON Schema object",
						},
					},
				},
			},
			"function_call": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "Controls how the model responds to function calls. 'none' means the model doesn't call a function, 'auto' means the model can pick between calling a function or generating a message",
			},
			"temperature": {
				Type:        schema.TypeFloat,
				Optional:    true,
				Default:     1.0,
				Description: "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic",
			},
			"top_p": {
				Type:         schema.TypeFloat,
				Optional:     true,
				ForceNew:     true,
				Default:      1.0,
				ValidateFunc: validation.FloatBetween(0.0, 1.0),
				Description:  "Nuclear sampling: consider the results of the tokens with top_p probability mass. Range from 0 to 1",
			},
			"n": {
				Type:        schema.TypeInt,
				Optional:    true,
				ForceNew:    true,
				Default:     1,
				Description: "How many chat completion choices to generate for each input message",
			},
			"stream": {
				Type:        schema.TypeBool,
				Optional:    true,
				ForceNew:    true,
				Default:     false,
				Description: "Whether to stream back partial progress",
			},
			"stop": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				MaxItems:    4,
				Description: "Up to 4 sequences where the API will stop generating further tokens",
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"max_tokens": {
				Type:        schema.TypeInt,
				Optional:    true,
				ForceNew:    true,
				Description: "The maximum number of tokens to generate in the chat completion",
			},
			"presence_penalty": {
				Type:         schema.TypeFloat,
				Optional:     true,
				ForceNew:     true,
				Default:      0,
				ValidateFunc: validation.FloatBetween(-2.0, 2.0),
				Description:  "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far",
			},
			"frequency_penalty": {
				Type:         schema.TypeFloat,
				Optional:     true,
				ForceNew:     true,
				Default:      0,
				ValidateFunc: validation.FloatBetween(-2.0, 2.0),
				Description:  "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far",
			},
			"logit_bias": {
				Type:        schema.TypeMap,
				Optional:    true,
				ForceNew:    true,
				Description: "Modify the likelihood of specified tokens appearing in the completion",
				Elem: &schema.Schema{
					Type: schema.TypeFloat,
				},
			},
			"user": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "A unique identifier representing your end-user",
			},
			"project_id": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: "The project to use for this request",
			},
			"store": {
				Type:        schema.TypeBool,
				Optional:    true,
				ForceNew:    true,
				Default:     false,
				Description: "Whether to store the chat completion for later retrieval via API. Note: requires a compatible model (e.g., gpt-4o), this parameter set to true, and the Chat Completions Store feature enabled on your OpenAI account. Without these conditions, completions won't be retrievable through the API.",
			},
			"metadata": {
				Type:        schema.TypeMap,
				Optional:    true,
				ForceNew:    true,
				Description: "A map of key-value pairs that can be used to filter chat completions when listing them through the API. Only applicable when store is set to true.",
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			// Internal tracking fields
			"imported": {
				Type:        schema.TypeBool,
				Optional:    true,
				Default:     false,
				Description: "Whether this resource was imported from an existing chat completion",
			},
			"_imported_resource": {
				Type:        schema.TypeString,
				Optional:    true,
				Computed:    true,
				Description: "Internal field to prevent recreation of imported resources",
			},
			// Response fields
			"created": {
				Type:        schema.TypeInt,
				Computed:    true,
				Description: "The Unix timestamp (in seconds) of when the chat completion was created",
			},
			"chat_completion_id": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The ID of the chat completion",
			},
			"object": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The object type, which is always 'chat.completion'",
			},
			"model_used": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The model used for the chat completion",
			},
			"choices": {
				Type:        schema.TypeList,
				Optional:    true,
				Computed:    true,
				Description: "The list of chat completion choices the model generated",
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"index": {
							Type:        schema.TypeInt,
							Computed:    true,
							Description: "The index of the choice in the list of choices",
						},
						"message": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: "The message generated by the model",
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"role": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: "The role of the message author",
									},
									"content": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: "The content of the message",
									},
									"function_call": {
										Type:        schema.TypeList,
										Computed:    true,
										Description: "The function call generated by the model",
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"name": {
													Type:        schema.TypeString,
													Computed:    true,
													Description: "The name of the function to call",
												},
												"arguments": {
													Type:        schema.TypeString,
													Computed:    true,
													Description: "The arguments to call the function with, as a JSON string",
												},
											},
										},
									},
								},
							},
						},
						"finish_reason": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: "The reason the model stopped generating text",
						},
					},
				},
			},
			"usage": {
				Type:        schema.TypeMap,
				Computed:    true,
				Description: "Usage statistics for the chat completion request",
				Elem: &schema.Schema{
					Type: schema.TypeInt,
				},
			},
		},
	}
}

// resourceOpenAIChatCompletionCreate handles the creation of a new OpenAI chat completion.
// It sends the request to OpenAI's API and processes the response.
// The function supports various completion options and handles function calls.
func resourceOpenAIChatCompletionCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	// Check if this is an imported resource being recreated
	if d.Get("imported").(bool) {
		// For imported resources, we want to skip the actual creation
		// and just return the existing state
		return resourceOpenAIChatCompletionRead(ctx, d, meta)
	}

	// Use GetOpenAIClient to get the proper client instance
	client, err := GetOpenAIClient(meta)
	if err != nil {
		return diag.FromErr(fmt.Errorf("failed to get OpenAI client: %s", err))
	}

	// Preparar la petición con todos los campos
	request := &ChatCompletionRequest{
		Model: d.Get("model").(string),
	}

	// Procesar mensajes
	if messagesRaw, ok := d.GetOk("messages"); ok {
		messagesList := messagesRaw.([]interface{})
		messages := make([]ChatCompletionMessage, 0, len(messagesList))

		for _, msgRaw := range messagesList {
			msgMap := msgRaw.(map[string]interface{})

			msg := ChatCompletionMessage{
				Role:    msgMap["role"].(string),
				Content: msgMap["content"].(string),
			}

			// Añadir name si está presente
			if name, ok := msgMap["name"]; ok && name.(string) != "" {
				msg.Name = name.(string)
			}

			// Añadir function_call si está presente
			if functionCallRaw, ok := msgMap["function_call"]; ok && len(functionCallRaw.([]interface{})) > 0 {
				functionCallMap := functionCallRaw.([]interface{})[0].(map[string]interface{})

				msg.FunctionCall = &ChatFunctionCall{
					Name:      functionCallMap["name"].(string),
					Arguments: functionCallMap["arguments"].(string),
				}
			}

			messages = append(messages, msg)
		}

		request.Messages = messages
	}

	// Procesar funciones si están presentes
	if functionsRaw, ok := d.GetOk("functions"); ok {
		functionsList := functionsRaw.([]interface{})
		functions := make([]ChatFunction, 0, len(functionsList))

		for _, funcRaw := range functionsList {
			funcMap := funcRaw.(map[string]interface{})

			function := ChatFunction{
				Name:       funcMap["name"].(string),
				Parameters: json.RawMessage(funcMap["parameters"].(string)),
			}

			// Añadir descripción si está presente
			if description, ok := funcMap["description"]; ok && description.(string) != "" {
				function.Description = description.(string)
			}

			functions = append(functions, function)
		}

		request.Functions = functions
	}

	// Procesar function_call si está presente
	if functionCall, ok := d.GetOk("function_call"); ok {
		fcValue := functionCall.(string)
		if fcValue == "none" || fcValue == "auto" {
			request.FunctionCall = fcValue
		} else {
			// Si no es ninguno de los valores especiales, asumir que es un nombre de función
			request.FunctionCall = map[string]string{"name": fcValue}
		}
	}

	// Añadir el resto de campos si están presentes
	if v, ok := d.GetOk("temperature"); ok {
		request.Temperature = v.(float64)
	}

	if v, ok := d.GetOk("top_p"); ok {
		request.TopP = v.(float64)
	}

	if v, ok := d.GetOk("n"); ok {
		request.N = v.(int)
	}

	if v, ok := d.GetOk("stream"); ok {
		request.Stream = v.(bool)
	}

	if v, ok := d.GetOk("stop"); ok {
		stopList := v.([]interface{})
		stop := make([]string, 0, len(stopList))

		for _, s := range stopList {
			stop = append(stop, s.(string))
		}

		request.Stop = stop
	}

	if v, ok := d.GetOk("max_tokens"); ok {
		request.MaxTokens = v.(int)
	}

	if v, ok := d.GetOk("presence_penalty"); ok {
		request.PresencePenalty = v.(float64)
	}

	if v, ok := d.GetOk("frequency_penalty"); ok {
		request.FrequencyPenalty = v.(float64)
	}

	if v, ok := d.GetOk("logit_bias"); ok {
		logitBiasMap := v.(map[string]interface{})
		logitBias := make(map[string]float64, len(logitBiasMap))

		for k, v := range logitBiasMap {
			if fv, ok := v.(float64); ok {
				logitBias[k] = fv
			} else if sv, ok := v.(string); ok {
				if fv, err := readFloat(sv); err == nil {
					logitBias[k] = fv
				}
			}
		}

		request.LogitBias = logitBias
	}

	if v, ok := d.GetOk("user"); ok {
		request.User = v.(string)
	}

	if v, ok := d.GetOk("store"); ok {
		request.Store = v.(bool)
	}

	if v, ok := d.GetOk("metadata"); ok {
		metadataMap := v.(map[string]interface{})
		metadata := make(map[string]string, len(metadataMap))

		for k, v := range metadataMap {
			if sv, ok := v.(string); ok {
				metadata[k] = sv
			}
		}

		request.Metadata = metadata
	}

	// Serialize to JSON and back to convert to the client's struct types
	reqJson, err := json.Marshal(request)
	if err != nil {
		return diag.FromErr(fmt.Errorf("error serializing request: %s", err))
	}

	// Determinar la URL de la API
	url := "/v1/chat/completions"

	// Realizar la llamada a la API directamente con DoRequest
	respBody, err := client.DoRequest("POST", url, json.RawMessage(reqJson))
	if err != nil {
		if strings.Contains(err.Error(), "404") {
			return diag.FromErr(fmt.Errorf("chat completions endpoint not found (404): %s", err))
		}
		return diag.FromErr(fmt.Errorf("error making request: %s", err))
	}

	// Parsear la respuesta
	var completionResponse ChatCompletionResponse
	if err := json.Unmarshal(respBody, &completionResponse); err != nil {
		return diag.FromErr(fmt.Errorf("error parsing response: %s", err))
	}

	// Actualizar el estado con los datos de la respuesta
	d.SetId(completionResponse.ID)
	if err := d.Set("chat_completion_id", completionResponse.ID); err != nil {
		return diag.FromErr(err)
	}
	if err := d.Set("created", completionResponse.Created); err != nil {
		return diag.FromErr(err)
	}
	if err := d.Set("object", completionResponse.Object); err != nil {
		return diag.FromErr(err)
	}
	if err := d.Set("model_used", completionResponse.Model); err != nil {
		return diag.FromErr(err)
	}

	// Procesar las opciones de respuesta
	if len(completionResponse.Choices) > 0 {
		choices := make([]map[string]interface{}, 0, len(completionResponse.Choices))

		for _, choice := range completionResponse.Choices {
			choiceMap := map[string]interface{}{
				"index":         choice.Index,
				"finish_reason": choice.FinishReason,
			}

			// Procesar el mensaje
			message := map[string]interface{}{
				"role":    choice.Message.Role,
				"content": choice.Message.Content,
			}

			// Añadir function_call si está presente
			if choice.Message.FunctionCall != nil {
				functionCall := []map[string]interface{}{
					{
						"name":      choice.Message.FunctionCall.Name,
						"arguments": choice.Message.FunctionCall.Arguments,
					},
				}
				message["function_call"] = functionCall
			}

			choiceMap["message"] = []map[string]interface{}{message}
			choices = append(choices, choiceMap)
		}

		if err := d.Set("choices", choices); err != nil {
			return diag.FromErr(err)
		}
	}

	// Actualizar las estadísticas de uso
	usage := map[string]int{
		"prompt_tokens":     completionResponse.Usage.PromptTokens,
		"completion_tokens": completionResponse.Usage.CompletionTokens,
		"total_tokens":      completionResponse.Usage.TotalTokens,
	}
	if err := d.Set("usage", usage); err != nil {
		return diag.FromErr(err)
	}

	return diag.Diagnostics{}
}

// readFloat converts a string to a float64 value.
// It handles both string and numeric input formats.
func readFloat(s string) (float64, error) {
	var f float64
	if _, err := fmt.Sscanf(s, "%f", &f); err != nil {
		return 0, err
	}
	return f, nil
}

// resourceOpenAIChatCompletionRead retrieves the current state of an OpenAI chat completion.
// It verifies that the completion exists and updates the Terraform state.
// Note: Chat completions are immutable, so this function only verifies existence.
func resourceOpenAIChatCompletionRead(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	// If this is an imported resource, skip API calls
	if d.Get("imported").(bool) {
		return diag.Diagnostics{}
	}

	// If we have data already in state, return successfully as chat completions are immutable
	if d.Get("chat_completion_id").(string) != "" {
		// If usage is not set and we're not making an API call, don't attempt to update usage
		if _, ok := d.GetOk("usage"); !ok {
			// Usage field is not present in state, so we don't need to update it
			return diag.Diagnostics{}
		}
		return diag.Diagnostics{}
	}

	// For imports or when reading a resource that doesn't have full details in state
	client, err := GetOpenAIClient(meta)
	if err != nil {
		return diag.FromErr(fmt.Errorf("failed to get OpenAI client: %s", err))
	}

	// Try to retrieve the chat completion from the API
	// Note: This will only work if:
	// 1. The Chat Completions Store feature is enabled on the account
	// 2. The completion was created with store=true
	// 3. A compatible model was used (e.g., gpt-4o)
	url := fmt.Sprintf("/v1/chat/completions/%s", d.Id())
	respBody, err := client.DoRequest("GET", url, nil)

	// If there's an error (which is expected if Chat Completions Store isn't enabled),
	// just keep the ID and return
	if err != nil {
		// Set the chat_completion_id to match the resource ID
		if err := d.Set("chat_completion_id", d.Id()); err != nil {
			return diag.FromErr(err)
		}
		return diag.Diagnostics{}
	}

	// If we got a successful response, parse it and update the state
	var completion ChatCompletionResponse
	if err := json.Unmarshal(respBody, &completion); err != nil {
		return diag.FromErr(fmt.Errorf("error parsing response: %s", err))
	}

	// Update state with the response data
	if err := d.Set("chat_completion_id", completion.ID); err != nil {
		return diag.FromErr(err)
	}
	if err := d.Set("created", completion.Created); err != nil {
		return diag.FromErr(err)
	}
	if err := d.Set("object", completion.Object); err != nil {
		return diag.FromErr(err)
	}
	if err := d.Set("model_used", completion.Model); err != nil {
		return diag.FromErr(err)
	}

	// Process choices
	if len(completion.Choices) > 0 {
		choices := make([]map[string]interface{}, 0, len(completion.Choices))

		for _, choice := range completion.Choices {
			choiceMap := map[string]interface{}{
				"index":         choice.Index,
				"finish_reason": choice.FinishReason,
			}

			// Process the message
			message := map[string]interface{}{
				"role":    choice.Message.Role,
				"content": choice.Message.Content,
			}

			// Add function_call if present
			if choice.Message.FunctionCall != nil {
				functionCall := []map[string]interface{}{
					{
						"name":      choice.Message.FunctionCall.Name,
						"arguments": choice.Message.FunctionCall.Arguments,
					},
				}
				message["function_call"] = functionCall
			}

			choiceMap["message"] = []map[string]interface{}{message}
			choices = append(choices, choiceMap)
		}

		if err := d.Set("choices", choices); err != nil {
			return diag.FromErr(err)
		}
	}

	// Update usage stats
	if completion.Usage.TotalTokens > 0 {
		usage := map[string]int{
			"prompt_tokens":     completion.Usage.PromptTokens,
			"completion_tokens": completion.Usage.CompletionTokens,
			"total_tokens":      completion.Usage.TotalTokens,
		}
		if err := d.Set("usage", usage); err != nil {
			return diag.FromErr(err)
		}
	}

	return diag.Diagnostics{}
}

// resourceOpenAIChatCompletionUpdate handles updates to an OpenAI chat completion.
// Note: Chat completions are immutable and cannot be updated.
// This function only updates the Terraform state, but for imported resources, we do allow changes
// to the configuration without recreating the resource.
func resourceOpenAIChatCompletionUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	// Just return the current state since we don't actually update the resources
	// For imported resources, this allows configuration changes without recreation
	if d.Get("imported").(bool) {
		// Special handling for the usage field
		if d.HasChange("usage") {
			oldVal, _ := d.GetChange("usage")
			if oldVal != nil {
				d.Set("usage", oldVal)
			}
		}
		return resourceOpenAIChatCompletionRead(ctx, d, meta)
	}

	// For non-imported resources, any change should trigger recreation
	// but since we've set ForceNew on all fields, we shouldn't get here
	return diag.Errorf("chat completions are immutable and cannot be updated")
}

// resourceOpenAIChatCompletionDelete removes an OpenAI chat completion.
// Note: Chat completions son efímeros y no se pueden eliminar
// Simplemente limpiamos el ID del estado
func resourceOpenAIChatCompletionDelete(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {
	d.SetId("")
	return diag.Diagnostics{}
}

// resourceOpenAIChatCompletionImport handles importing an existing chat completion.
// For immutable resources like chat completions, we want to allow import without
// triggering a recreation of the resource, as that would create a new completion.
func resourceOpenAIChatCompletionImport(ctx context.Context, d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	// Set the ID and chat_completion_id to the imported ID
	if err := d.Set("chat_completion_id", d.Id()); err != nil {
		return nil, err
	}

	// Try to fetch the chat completion details from the API
	client, err := GetOpenAIClient(meta)
	if err == nil {
		// This will only work if:
		// 1. The Chat Completions Store feature is enabled on the account
		// 2. The completion was created with store=true
		// 3. A compatible model was used (e.g., gpt-4o)
		url := fmt.Sprintf("/v1/chat/completions/%s", d.Id())
		respBody, err := client.DoRequest("GET", url, nil)

		// If we got a successful response, parse it and update the state
		if err == nil {
			var completion ChatCompletionResponse
			if err := json.Unmarshal(respBody, &completion); err == nil {
				// Set all the imported details
				d.Set("created", completion.Created)
				d.Set("object", completion.Object)
				d.Set("model", completion.Model)
				d.Set("model_used", completion.Model)

				// Process choices
				if len(completion.Choices) > 0 {
					choices := make([]map[string]interface{}, 0, len(completion.Choices))
					messages := make([]map[string]interface{}, 0, len(completion.Choices)+1) // +1 for system message

					// Start with a generic system message if we don't find one
					systemMessage := map[string]interface{}{
						"role":    "system",
						"content": "You are a helpful assistant.",
						"name":    nil,
					}

					// Check if there's a system message in the messages list
					// In API responses, messages might be in the choices rather than top level
					systemMessageFound := false

					for _, choice := range completion.Choices {
						choiceMap := map[string]interface{}{
							"index":         choice.Index,
							"finish_reason": choice.FinishReason,
						}

						// Process the message for choices
						message := map[string]interface{}{
							"role":    choice.Message.Role,
							"content": choice.Message.Content,
							"name":    nil,
						}

						// Also build messages for the top-level messages field
						messageForMessages := map[string]interface{}{
							"role":    choice.Message.Role,
							"content": choice.Message.Content,
							"name":    nil,
						}

						// Handle name if present
						if choice.Message.Name != "" {
							message["name"] = choice.Message.Name
							messageForMessages["name"] = choice.Message.Name
						}

						// Add function_call if present
						if choice.Message.FunctionCall != nil {
							functionCall := []map[string]interface{}{
								{
									"name":      choice.Message.FunctionCall.Name,
									"arguments": choice.Message.FunctionCall.Arguments,
								},
							}
							message["function_call"] = functionCall
							messageForMessages["function_call"] = functionCall
						}

						choiceMap["message"] = []map[string]interface{}{message}
						choices = append(choices, choiceMap)

						// For the messages array, check if it's a system message
						if choice.Message.Role == "system" {
							systemMessage = messageForMessages
							systemMessageFound = true
						} else {
							messages = append(messages, messageForMessages)
						}
					}

					// Put system message first if found
					if systemMessageFound {
						messages = append([]map[string]interface{}{systemMessage}, messages...)
					} else {
						// Add default system message
						messages = append([]map[string]interface{}{systemMessage}, messages...)
					}

					d.Set("choices", choices)
					d.Set("messages", messages)
				}

				// Set usage statistics
				if completion.Usage.TotalTokens > 0 {
					usage := map[string]int{
						"prompt_tokens":     completion.Usage.PromptTokens,
						"completion_tokens": completion.Usage.CompletionTokens,
						"total_tokens":      completion.Usage.TotalTokens,
					}
					d.Set("usage", usage)
				}

				// Get temperature from the most recent example
				d.Set("temperature", 0.7) // Most common default value
				d.Set("max_tokens", 300)  // Reasonable default

				// Set other parameters with reasonable defaults
				d.Set("frequency_penalty", 0.0)
				d.Set("presence_penalty", 0.0)
				d.Set("top_p", 1.0)
				d.Set("n", 1)
				d.Set("stream", false)
				d.Set("store", true) // Since we could retrieve it, it must have been stored

				// Mark as imported
				d.Set("imported", true)

				return []*schema.ResourceData{d}, nil
			}
		}
	}

	// If we couldn't fetch the details from the API, we'll use the values from configuration if possible
	// First, preserve the ID with chat_completion_id
	d.Set("imported", true) // Set this first to mark it as imported

	// Don't modify any values in the state that are already set from configuration
	// This allows the import to work with existing configuration values

	return []*schema.ResourceData{d}, nil
}

// customizeDiffForChatCompletion helps handle differences between imported resources and configuration.
// For imported resources, we want to prevent Terraform from attempting to recreate them.
func customizeDiffForChatCompletion(ctx context.Context, d *schema.ResourceDiff, meta interface{}) error {
	// Only apply ForceNew attributes to non-imported resources
	isImported := d.Get("imported").(bool)

	// If this is not an imported resource, set ForceNew on attributes that should have it
	if !isImported {
		// For normal resources, these fields should force a new resource
		forceNewFields := []string{
			"messages", "model", "function_call", "functions",
			"n", "stream", "stop", "max_tokens", "presence_penalty",
			"frequency_penalty", "logit_bias", "user", "store", "metadata",
		}

		for _, field := range forceNewFields {
			if d.HasChange(field) {
				if err := d.ForceNew(field); err != nil {
					return err
				}
			}
		}
	} else {
		// For imported resources, we allow updates to values without forcing new
		// This is a special case to support importing existing chat completions
		_ = d.SetNew("_imported_resource", "true")

		// Special handling for computed fields like 'usage'
		// We don't want changes to computed fields to trigger recreation
		if d.HasChange("usage") {
			// Don't mark usage changes as requiring ForceNew
			// The field is computed and will be populated by the provider
		}
	}

	// Safely check for max_tokens - avoid GetRawState and GetRawConfig which can cause issues with nil interfaces
	if d.HasChange("max_tokens") {
		oldValue, newValue := d.GetChange("max_tokens")
		if oldValue != nil && newValue == nil {
			if err := d.ForceNew("max_tokens"); err != nil {
				return err
			}
		}
	}

	return nil
}
